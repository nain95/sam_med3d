"""
ê°œì„ ëœ í›ˆë ¨ í•¨ìˆ˜ë“¤
- Mixed precision training
- Gradient accumulation
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
"""

import torch
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import time
import numpy as np
from trainer_improved import (
    calculate_dice_safe, calculate_iou_safe, safe_tensor_operations,
    log_sample_predictions, log_metrics_to_wandb, get_memory_info, cleanup_memory
)


def train_epoch_improved(model, dataloader, optimizer, scaler, device, config, epoch):
    """ê°œì„ ëœ í›ˆë ¨ ì—í¬í¬ - Mixed precision ë° Gradient accumulation ì§€ì›"""
    model.train()
    
    total_loss = 0
    total_dice = 0
    total_iou = 0
    processed_batches = 0
    accumulated_batches = 0
    
    # Loss ì»´í¬ë„ŒíŠ¸ë³„ ì¶”ì 
    loss_components = {
        'dice_loss': 0,
        'focal_loss': 0,
        'iou_loss': 0,
        'iou_pred_loss': 0,
        'representation_loss': 0
    }
    
    pbar = tqdm(dataloader, desc=f"Training Epoch {epoch+1}")
    start_time = time.time()
    
    # Gradient accumulationì„ ìœ„í•œ ì´ˆê¸°í™”
    optimizer.zero_grad()
    
    for batch_idx, batch in enumerate(pbar):
        if batch is None or batch["patient_id"][0] == "dummy":
            continue
            
        try:
            image = batch["image"].to(device, non_blocking=True)
            mask = batch["mask"].to(device, non_blocking=True)
            
            # í…ì„œ ì—°ì†ì„± ë³´ì¥
            image = image.contiguous()
            mask = mask.contiguous()
            
            if image.shape[0] == 0 or mask.shape[0] == 0:
                continue
            
            # Mixed precision forward pass
            with autocast(enabled=config.use_mixed_precision):
                # Forward pass
                results = model(image, mask)
                pred_masks = results['pred_masks']
                
                if not pred_masks.is_contiguous():
                    pred_masks = pred_masks.contiguous()
                
                # Loss ê³„ì‚°
                loss_weights = {
                    'dice': config.dice_weight,
                    'focal': config.focal_weight,
                    'iou': config.iou_weight,
                    'iou_pred': config.iou_pred_weight,
                    'representation': config.repr_weight
                }
                
                losses = model.calculate_loss(
                    pred_masks, 
                    mask,
                    results.get('iou_predictions'),
                    results.get('representation_features'),
                    loss_weights=loss_weights
                )
                
                # Gradient accumulationì„ ìœ„í•œ loss scaling
                total_loss_batch = losses['total_loss'] / config.gradient_accumulation_steps
            
            # Backward pass with mixed precision
            if config.use_mixed_precision:
                scaler.scale(total_loss_batch).backward()
            else:
                total_loss_batch.backward()
            
            accumulated_batches += 1
            
            # Gradient accumulation ì²´í¬
            if accumulated_batches >= config.gradient_accumulation_steps:
                # Gradient clipping and optimization step
                if config.use_mixed_precision:
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
                    optimizer.step()
                
                optimizer.zero_grad()
                accumulated_batches = 0
            
            # Metrics ê³„ì‚° (GPUì—ì„œ ì§ì ‘)
            with torch.no_grad():
                pred_masks_detached = pred_masks.detach()
                mask_detached = mask.detach()
                
                pred_masks_detached, mask_detached = safe_tensor_operations(
                    pred_masks_detached, mask_detached
                )
                
                dice = calculate_dice_safe(pred_masks_detached, mask_detached)
                iou = calculate_iou_safe(pred_masks_detached, mask_detached)
            
            # ëˆ„ì 
            total_loss += losses['total_loss'].item()
            total_dice += dice
            total_iou += iou
            processed_batches += 1
            
            # Loss ì»´í¬ë„ŒíŠ¸ ëˆ„ì 
            for key in loss_components:
                if key in losses:
                    loss_components[key] += losses[key].item()
            
            # Progress bar ì—…ë°ì´íŠ¸
            current_lr = optimizer.param_groups[0]['lr']
            pbar.set_postfix({
                'Loss': f'{total_loss / processed_batches:.3f}',
                'Dice': f'{total_dice / processed_batches:.3f}',
                'IoU': f'{total_iou / processed_batches:.3f}',
                'LR': f'{current_lr:.2e}',
                'Mem': f'{torch.cuda.memory_allocated() / 1e9:.1f}GB' if torch.cuda.is_available() else 'N/A'
            })
            
            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
            if batch_idx % 50 == 0:
                cleanup_memory()
            
            # í”„ë¦°íŠ¸ ì£¼ê¸°
            if processed_batches % config.print_freq == 0:
                elapsed = time.time() - start_time
                print(f"\n  ë°°ì¹˜ {processed_batches}/{len(dataloader)} - "
                      f"Loss: {total_loss / processed_batches:.4f}, "
                      f"ì‹œê°„: {elapsed:.1f}s")
                
        except RuntimeError as e:
            if "out of memory" in str(e):
                print(f"\nâŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (ë°°ì¹˜ {batch_idx}): {e}")
                print(f"ğŸ’¡ í˜„ì¬ ë°°ì¹˜ í¬ê¸°: {image.shape[0]}, ì´ë¯¸ì§€ í¬ê¸°: {image.shape}")
                cleanup_memory()
                
                # ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ í•´ë‹¹ ë°°ì¹˜ ìŠ¤í‚µ
                if 'image' in locals():
                    del image
                if 'mask' in locals():
                    del mask
                if 'results' in locals():
                    del results
                continue
            else:
                print(f"âŒ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ {batch_idx} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue
    
    # ë‚¨ì€ gradientê°€ ìˆë‹¤ë©´ ì—…ë°ì´íŠ¸
    if accumulated_batches > 0:
        if config.use_mixed_precision:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
            scaler.step(optimizer)
            scaler.update()
        else:
            torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)
            optimizer.step()
        optimizer.zero_grad()
    
    if processed_batches == 0:
        print("âš ï¸ ì²˜ë¦¬ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return 0.0, 0.0, 0.0, {}
    
    # í‰ê·  ê³„ì‚°
    avg_loss = total_loss / processed_batches
    avg_dice = total_dice / processed_batches
    avg_iou = total_iou / processed_batches
    
    # Loss ì»´í¬ë„ŒíŠ¸ í‰ê· 
    avg_loss_components = {
        key: value / processed_batches 
        for key, value in loss_components.items()
    }
    
    return avg_loss, avg_dice, avg_iou, avg_loss_components


def validate_epoch_improved(model, dataloader, device, config, epoch):
    """ê°œì„ ëœ ê²€ì¦ ì—í¬í¬ - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬"""
    model.eval()
    
    total_loss = 0
    total_dice = 0
    total_iou = 0
    processed_batches = 0
    
    # Loss ì»´í¬ë„ŒíŠ¸ë³„ ì¶”ì 
    loss_components = {
        'dice_loss': 0,
        'focal_loss': 0,
        'iou_loss': 0,
        'iou_pred_loss': 0,
        'representation_loss': 0
    }
    
    with torch.no_grad():
        pbar = tqdm(dataloader, desc=f"Validation Epoch {epoch+1}")
        start_time = time.time()
        
        for batch_idx, batch in enumerate(pbar):
            if batch is None or batch["patient_id"][0] == "dummy":
                continue
                
            try:
                image = batch["image"].to(device, non_blocking=True)
                mask = batch["mask"].to(device, non_blocking=True)
                
                # í…ì„œ ì—°ì†ì„± ë³´ì¥
                image = image.contiguous()
                mask = mask.contiguous()
                
                if image.shape[0] == 0 or mask.shape[0] == 0:
                    continue
                
                # Mixed precision forward pass
                with autocast(enabled=config.use_mixed_precision):
                    # Forward pass
                    results = model(image, mask)
                    pred_masks = results['pred_masks']
                    
                    if not pred_masks.is_contiguous():
                        pred_masks = pred_masks.contiguous()
                    
                    # Loss ê³„ì‚°
                    loss_weights = {
                        'dice': config.dice_weight,
                        'focal': config.focal_weight,
                        'iou': config.iou_weight,
                        'iou_pred': config.iou_pred_weight,
                        'representation': config.repr_weight
                    }
                    
                    losses = model.calculate_loss(
                        pred_masks,
                        mask,
                        results.get('iou_predictions'),
                        results.get('representation_features'),
                        loss_weights=loss_weights
                    )
                
                # Metrics ê³„ì‚°
                pred_masks, mask = safe_tensor_operations(pred_masks, mask)
                dice = calculate_dice_safe(pred_masks, mask)
                iou = calculate_iou_safe(pred_masks, mask)
                
                # ëˆ„ì 
                total_loss += losses['total_loss'].item()
                total_dice += dice
                total_iou += iou
                processed_batches += 1
                
                # Loss ì»´í¬ë„ŒíŠ¸ ëˆ„ì 
                for key in loss_components:
                    if key in losses:
                        loss_components[key] += losses[key].item()
                
                # Progress bar ì—…ë°ì´íŠ¸
                pbar.set_postfix({
                    'Loss': f'{total_loss / processed_batches:.3f}',
                    'Dice': f'{total_dice / processed_batches:.3f}',
                    'IoU': f'{total_iou / processed_batches:.3f}',
                    'Mem': f'{torch.cuda.memory_allocated() / 1e9:.1f}GB' if torch.cuda.is_available() else 'N/A'
                })
                
                # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
                if batch_idx % 20 == 0:
                    cleanup_memory()
                
            except RuntimeError as e:
                if "out of memory" in str(e):
                    print(f"\nâŒ ê²€ì¦ ì¤‘ GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (ë°°ì¹˜ {batch_idx}): {e}")
                    cleanup_memory()
                    continue
                else:
                    print(f"âŒ ê²€ì¦ ë°°ì¹˜ {batch_idx} ì‹¤íŒ¨: {e}")
                    continue
            except Exception as e:
                print(f"âŒ ê²€ì¦ ë°°ì¹˜ {batch_idx} ì‹¤íŒ¨: {e}")
                continue
    
    if processed_batches == 0:
        print("âš ï¸ ê²€ì¦ì—ì„œ ì²˜ë¦¬ëœ ë°°ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
        return 0.0, 0.0, 0.0, {}
    
    # í‰ê·  ê³„ì‚°
    avg_loss = total_loss / processed_batches
    avg_dice = total_dice / processed_batches
    avg_iou = total_iou / processed_batches
    
    # Loss ì»´í¬ë„ŒíŠ¸ í‰ê· 
    avg_loss_components = {
        key: value / processed_batches 
        for key, value in loss_components.items()
    }
    
    return avg_loss, avg_dice, avg_iou, avg_loss_components


def save_checkpoint_improved(model, optimizer, scheduler, scaler, epoch, metrics, config, 
                            checkpoint_type="regular"):
    """ê°œì„ ëœ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    try:
        checkpoint_data = {
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
            'scaler_state_dict': scaler.state_dict() if scaler else None,
            'metrics': metrics,
            'config_dict': config.to_dict(),
            'checkpoint_type': checkpoint_type,
            'pytorch_version': torch.__version__
        }
        
        # íŒŒì¼ëª… ì„¤ì •
        if checkpoint_type == "best":
            filename = "best_model.pth"
        elif checkpoint_type == "latest":
            filename = "latest_model.pth"
        elif checkpoint_type == "sam_encoder":
            filename = "sam_encoder.pth"
        else:
            filename = f"checkpoint_epoch_{epoch:03d}.pth"
        
        filepath = os.path.join(config.output_dir, filename)
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(config.output_dir, exist_ok=True)
        
        # ì €ì¥
        torch.save(checkpoint_data, filepath)
        
        # SAM encoderë§Œ ë³„ë„ ì €ì¥ (BLIP2 í˜¸í™˜)
        if checkpoint_type == "best":
            try:
                sam_encoder_state = {
                    'image_encoder_state_dict': model.sam.image_encoder.state_dict(),
                    'metrics': metrics,
                    'config_dict': config.to_dict(),
                    'epoch': epoch
                }
                sam_encoder_path = os.path.join(config.output_dir, 'sam_encoder.pth')
                torch.save(sam_encoder_state, sam_encoder_path)
                print(f"ğŸ’¾ SAM encoder ì €ì¥: {sam_encoder_path}")
            except Exception as e:
                print(f"âš ï¸ SAM encoder ì €ì¥ ì‹¤íŒ¨: {e}")
        
        print(f"ğŸ’¾ ì²´í¬í¬ì¸íŠ¸ ì €ì¥: {filepath}")
        return filepath
        
    except Exception as e:
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None


def load_checkpoint_improved(checkpoint_path, model, optimizer=None, scheduler=None, 
                           scaler=None, device='cuda'):
    """ê°œì„ ëœ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    try:
        print(f"ğŸ“‚ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì¤‘: {checkpoint_path}")
        
        checkpoint = torch.load(checkpoint_path, map_location=device)
        
        # ëª¨ë¸ ìƒíƒœ ë¡œë“œ
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # ì˜µí‹°ë§ˆì´ì € ìƒíƒœ ë¡œë“œ
        if optimizer is not None and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ ë¡œë“œ
        if scheduler is not None and 'scheduler_state_dict' in checkpoint:
            if checkpoint['scheduler_state_dict'] is not None:
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        # Scaler ìƒíƒœ ë¡œë“œ
        if scaler is not None and 'scaler_state_dict' in checkpoint:
            if checkpoint['scaler_state_dict'] is not None:
                scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        # ë©”íƒ€ë°ì´í„° ë°˜í™˜
        metadata = {
            'epoch': checkpoint.get('epoch', 0),
            'metrics': checkpoint.get('metrics', {}),
            'config': checkpoint.get('config_dict', {})
        }
        
        print(f"âœ… ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì™„ë£Œ (Epoch: {metadata['epoch']})")
        
        return metadata
        
    except Exception as e:
        print(f"âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None
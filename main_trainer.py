"""
ê°œì„ ëœ ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜
- ì•ˆì •ì ì¸ í›ˆë ¨ ë£¨í”„
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±
- ì™„ì „í•œ ëª¨ë‹ˆí„°ë§
"""

import torch
from torch.cuda.amp import GradScaler
import time
import os
from pathlib import Path
import wandb

from trainer_improved import (
    EarlyStopping, create_optimizer, create_scheduler,
    log_sample_predictions, log_metrics_to_wandb,
    get_memory_info, cleanup_memory, print_training_progress
)
from training_functions import (
    train_epoch_improved, validate_epoch_improved,
    save_checkpoint_improved, load_checkpoint_improved
)


def train_model_improved(model, train_loader, val_loader, config):
    """ê°œì„ ëœ ë©”ì¸ í›ˆë ¨ í•¨ìˆ˜"""
    
    print("\n" + "="*80)
    print("ğŸš€ ê°œì„ ëœ SAM-Med3D í›ˆë ¨ ì‹œì‘")
    print("="*80)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = torch.device(config.device)
    model = model.to(device)
    
    # WandB ì´ˆê¸°í™”
    wandb_run = None
    if config.use_wandb:
        try:
            wandb_run = config.init_wandb()
            if wandb_run:
                wandb.watch(model, log_freq=100, log_graph=False)  # graph ë¡œê¹… ë¹„í™œì„±í™” (ë©”ëª¨ë¦¬ ì ˆì•½)
        except Exception as e:
            print(f"âš ï¸ WandB ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            config.use_wandb = False
    
    # Mixed precision scaler
    scaler = GradScaler() if config.use_mixed_precision else None
    
    # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
    try:
        optimizer = create_optimizer(model, config)
        scheduler = create_scheduler(optimizer, config)
        
        print(f"âœ… ì˜µí‹°ë§ˆì´ì € ìƒì„± ì™„ë£Œ")
        print(f"   í•™ìŠµë¥ : {config.learning_rate}")
        print(f"   ê°€ì¤‘ì¹˜ ê°ì‡ : {config.weight_decay}")
        print(f"   Gradient accumulation: {config.gradient_accumulation_steps}")
        print(f"   Mixed precision: {config.use_mixed_precision}")
        
    except Exception as e:
        print(f"âŒ ì˜µí‹°ë§ˆì´ì € ìƒì„± ì‹¤íŒ¨: {e}")
        return None
    
    # Early stopping
    early_stopping = EarlyStopping(
        patience=config.patience,
        min_delta=0.001,
        restore_best_weights=True
    )
    
    # í›ˆë ¨ ìƒíƒœ ë³€ìˆ˜
    best_dice = 0
    best_iou = 0
    best_metrics = {}
    
    # ê²°ê³¼ ì €ì¥ìš©
    train_history = {'loss': [], 'dice': [], 'iou': []}
    val_history = {'loss': [], 'dice': [], 'iou': []}
    
    # ì‹œì‘ ì‹œê°„
    training_start_time = time.time()
    
    # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì •ë³´
    param_info = model.get_trainable_params()
    print(f"\nğŸ§  ëª¨ë¸ ì •ë³´:")
    print(f"   ì „ì²´ íŒŒë¼ë¯¸í„°: {param_info['total_params']:,}")
    print(f"   í•™ìŠµ ê°€ëŠ¥: {param_info['trainable_params']:,}")
    print(f"   í•™ìŠµ ê°€ëŠ¥ ë¹„ìœ¨: {param_info['trainable_ratio']:.1%}")
    
    # ì´ˆê¸° ë©”ëª¨ë¦¬ ìƒíƒœ
    initial_memory = get_memory_info()
    if initial_memory.get("available", False):
        print(f"   ì´ˆê¸° GPU ë©”ëª¨ë¦¬: {initial_memory['allocated_gb']:.1f}GB")
    
    print(f"\nğŸ“Š í›ˆë ¨ ì„¤ì •:")
    print(f"   ì—í¬í¬: {config.epochs}")
    print(f"   ë°°ì¹˜ í¬ê¸°: {config.batch_size}")
    print(f"   í›ˆë ¨ ìƒ˜í”Œ: {len(train_loader.dataset)}")
    print(f"   ê²€ì¦ ìƒ˜í”Œ: {len(val_loader.dataset)}")
    print(f"   ì¶œë ¥ ë””ë ‰í† ë¦¬: {config.output_dir}")
    
    # ë©”ì¸ í›ˆë ¨ ë£¨í”„
    try:
        for epoch in range(config.epochs):
            epoch_start_time = time.time()
            
            print(f"\n{'='*60}")
            print(f"ğŸ“… Epoch {epoch+1}/{config.epochs}")
            print(f"{'='*60}")
            
            # í˜„ì¬ í•™ìŠµë¥  ì¶œë ¥
            current_lrs = [group['lr'] for group in optimizer.param_groups]
            print(f"ğŸ“ˆ í•™ìŠµë¥ : {current_lrs[0]:.2e}")
            if len(current_lrs) > 1:
                print(f"   (representation head: {current_lrs[0]:.2e}, others: {current_lrs[1]:.2e})")
            
            # í›ˆë ¨
            print("\nğŸš‚ í›ˆë ¨ ì¤‘...")
            train_loss, train_dice, train_iou, train_loss_components = train_epoch_improved(
                model, train_loader, optimizer, scaler, device, config, epoch
            )
            
            # ê²€ì¦
            print("\nğŸ” ê²€ì¦ ì¤‘...")
            val_loss, val_dice, val_iou, val_loss_components = validate_epoch_improved(
                model, val_loader, device, config, epoch
            )
            
            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            if scheduler is not None:
                if config.scheduler_type == "reduce_on_plateau":
                    scheduler.step(val_dice)
                else:
                    scheduler.step()
            
            epoch_end_time = time.time()
            epoch_duration = epoch_end_time - epoch_start_time
            
            # íˆìŠ¤í† ë¦¬ ì €ì¥
            train_history['loss'].append(train_loss)
            train_history['dice'].append(train_dice)
            train_history['iou'].append(train_iou)
            
            val_history['loss'].append(val_loss)
            val_history['dice'].append(val_dice)
            val_history['iou'].append(val_iou)
            
            # í˜„ì¬ ì—í¬í¬ ë©”íŠ¸ë¦­
            current_metrics = {
                'train': {
                    'loss': train_loss,
                    'dice': train_dice,
                    'iou': train_iou,
                    **{f"loss_{k}": v for k, v in train_loss_components.items()}
                },
                'val': {
                    'loss': val_loss,
                    'dice': val_dice,
                    'iou': val_iou,
                    **{f"loss_{k}": v for k, v in val_loss_components.items()}
                }
            }
            
            # WandB ë¡œê¹…
            if config.use_wandb:
                try:
                    # ê¸°ë³¸ ë©”íŠ¸ë¦­
                    log_metrics_to_wandb(current_metrics['train'], epoch, "train")
                    log_metrics_to_wandb(current_metrics['val'], epoch, "val")
                    
                    # ì¶”ê°€ ë©”íŠ¸ë¦­
                    wandb.log({
                        'epoch': epoch,
                        'learning_rate': current_lrs[0],
                        'epoch_duration': epoch_duration,
                        'best_dice': best_dice,
                        'best_iou': best_iou
                    }, step=epoch)
                    
                    # GPU ë©”ëª¨ë¦¬ ë¡œê¹…
                    memory_info = get_memory_info()
                    if memory_info.get("available", False):
                        wandb.log({
                            'gpu_memory_allocated_gb': memory_info['allocated_gb'],
                            'gpu_memory_cached_gb': memory_info['cached_gb']
                        }, step=epoch)
                        
                except Exception as e:
                    print(f"âš ï¸ WandB ë¡œê¹… ì‹¤íŒ¨: {e}")
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            memory_info = get_memory_info()
            print_training_progress(epoch, config.epochs, current_metrics, epoch_duration, memory_info)
            
            # ìµœê³  ì„±ëŠ¥ ì²´í¬
            is_best_dice = val_dice > best_dice
            is_best_iou = val_iou > best_iou
            is_best = is_best_dice or is_best_iou
            
            if is_best:
                if is_best_dice:
                    best_dice = val_dice
                    print(f"ğŸ¯ ìƒˆë¡œìš´ ìµœê³  Dice: {best_dice:.4f}")
                
                if is_best_iou:
                    best_iou = val_iou
                    print(f"ğŸ¯ ìƒˆë¡œìš´ ìµœê³  IoU: {best_iou:.4f}")
                
                best_metrics = current_metrics
                
                # ìµœê³  ëª¨ë¸ ì €ì¥
                best_checkpoint_path = save_checkpoint_improved(
                    model, optimizer, scheduler, scaler, epoch, 
                    current_metrics, config, "best"
                )
                
                if best_checkpoint_path:
                    print(f"ğŸ’¾ ìµœê³  ëª¨ë¸ ì €ì¥: {best_checkpoint_path}")
            
            # ìµœì‹  ëª¨ë¸ ì €ì¥ (ì£¼ê¸°ì )
            if (epoch + 1) % 10 == 0:
                latest_checkpoint_path = save_checkpoint_improved(
                    model, optimizer, scheduler, scaler, epoch, 
                    current_metrics, config, "latest"
                )
            
            # ì´ë¯¸ì§€ ë¡œê¹… (ì£¼ê¸°ì )
            if config.use_wandb and config.log_images and (epoch + 1) % config.log_freq == 0:
                print("ğŸ“¸ ìƒ˜í”Œ ì˜ˆì¸¡ ë¡œê¹… ì¤‘...")
                log_sample_predictions(model, val_loader, device, config, epoch, "val")
            
            # Early stopping ì²´í¬
            if early_stopping(val_dice, model):
                print(f"\nğŸ›‘ Early stopping ë°œë™ (epoch {epoch+1})")
                print(f"   ìµœê³  Dice: {best_dice:.4f}")
                print(f"   ìµœê³  IoU: {best_iou:.4f}")
                break
            
            # ë©”ëª¨ë¦¬ ì •ë¦¬
            cleanup_memory()
            
            # í›ˆë ¨ ì‹œê°„ ì¶”ì •
            elapsed_time = time.time() - training_start_time
            avg_epoch_time = elapsed_time / (epoch + 1)
            remaining_epochs = config.epochs - epoch - 1
            estimated_remaining = avg_epoch_time * remaining_epochs
            
            print(f"â±ï¸ ê²½ê³¼ ì‹œê°„: {elapsed_time/3600:.1f}ì‹œê°„, "
                  f"ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {estimated_remaining/3600:.1f}ì‹œê°„")
        
        # í›ˆë ¨ ì™„ë£Œ
        total_training_time = time.time() - training_start_time
        
        print(f"\n{'='*80}")
        print("ğŸ‰ í›ˆë ¨ ì™„ë£Œ!")
        print(f"{'='*80}")
        print(f"   ì´ í›ˆë ¨ ì‹œê°„: {total_training_time/3600:.2f}ì‹œê°„")
        print(f"   ìµœê³  Dice: {best_dice:.4f}")
        print(f"   ìµœê³  IoU: {best_iou:.4f}")
        print(f"   ì €ì¥ ìœ„ì¹˜: {config.output_dir}")
        
        # ìµœì¢… ê²°ê³¼ ìš”ì•½
        summary = {
            'best_dice': best_dice,
            'best_iou': best_iou,
            'total_epochs': epoch + 1,
            'training_time_hours': total_training_time / 3600,
            'final_train_loss': train_history['loss'][-1] if train_history['loss'] else 0,
            'final_val_loss': val_history['loss'][-1] if val_history['loss'] else 0
        }
        
        # WandB ìµœì¢… ìš”ì•½
        if config.use_wandb:
            try:
                wandb.log({
                    'training_summary/best_dice': best_dice,
                    'training_summary/best_iou': best_iou,
                    'training_summary/total_epochs': epoch + 1,
                    'training_summary/training_time_hours': total_training_time / 3600
                })
                
                # í›ˆë ¨ ê³¡ì„  ì €ì¥
                wandb.log({
                    'training_curves/train_loss': train_history['loss'],
                    'training_curves/val_loss': val_history['loss'],
                    'training_curves/train_dice': train_history['dice'],
                    'training_curves/val_dice': val_history['dice']
                })
                
                print(f"ğŸ“ˆ WandB ë§í¬: {wandb.run.url}")
                
            except Exception as e:
                print(f"âš ï¸ WandB ìµœì¢… ë¡œê¹… ì‹¤íŒ¨: {e}")
        
        return {
            'model': model,
            'best_metrics': best_metrics,
            'summary': summary,
            'train_history': train_history,
            'val_history': val_history
        }
        
    except KeyboardInterrupt:
        print(f"\nâ¹ï¸ ì‚¬ìš©ìì— ì˜í•´ í›ˆë ¨ì´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤ (Epoch {epoch+1})")
        
        # ì¤‘ë‹¨ ì‹œì ì˜ ëª¨ë¸ ì €ì¥
        interrupt_checkpoint_path = save_checkpoint_improved(
            model, optimizer, scheduler, scaler, epoch, 
            current_metrics, config, f"interrupted_epoch_{epoch}"
        )
        
        if interrupt_checkpoint_path:
            print(f"ğŸ’¾ ì¤‘ë‹¨ ì‹œì  ëª¨ë¸ ì €ì¥: {interrupt_checkpoint_path}")
        
        return None
        
    except Exception as e:
        print(f"\nâŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        
        return None
        
    finally:
        # ì •ë¦¬ ì‘ì—…
        cleanup_memory()
        
        if config.use_wandb and wandb.run:
            try:
                wandb.finish()
                print("ğŸ“ WandB ì„¸ì…˜ ì¢…ë£Œ")
            except:
                pass


def resume_training_improved(model, checkpoint_path, train_loader, val_loader, config):
    """ê°œì„ ëœ í›ˆë ¨ ì¬ê°œ"""
    print(f"ğŸ”„ í›ˆë ¨ ì¬ê°œ: {checkpoint_path}")
    
    device = torch.device(config.device)
    model = model.to(device)
    
    # Mixed precision scaler
    scaler = GradScaler() if config.use_mixed_precision else None
    
    # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
    optimizer = create_optimizer(model, config)
    scheduler = create_scheduler(optimizer, config)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
    metadata = load_checkpoint_improved(
        checkpoint_path, model, optimizer, scheduler, scaler, device
    )
    
    if metadata is None:
        print("âŒ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ì‹¤íŒ¨")
        return None
    
    start_epoch = metadata['epoch'] + 1
    print(f"   ì‹œì‘ ì—í¬í¬: {start_epoch}")
    
    # ë‚¨ì€ ì—í¬í¬ë¡œ ì„¤ì • ì—…ë°ì´íŠ¸
    remaining_epochs = max(0, config.epochs - start_epoch)
    if remaining_epochs == 0:
        print("âš ï¸ ì´ë¯¸ í›ˆë ¨ì´ ì™„ë£Œëœ ëª¨ë¸ì…ë‹ˆë‹¤.")
        return model
    
    print(f"   ë‚¨ì€ ì—í¬í¬: {remaining_epochs}")
    
    # ê¸°ì¡´ ì„¤ì • ì¼ë¶€ ë³µì›
    if 'config' in metadata and metadata['config']:
        # ì¤‘ìš”í•œ ì„¤ì •ë“¤ì€ ìœ ì§€í•˜ë˜, ê²½ë¡œ ê´€ë ¨ ì„¤ì •ì€ í˜„ì¬ config ì‚¬ìš©
        old_config = metadata['config']
        for key in ['dice_weight', 'focal_weight', 'iou_weight', 'iou_pred_weight', 'repr_weight']:
            if key in old_config:
                setattr(config, key, old_config[key])
    
    # ì„¤ì • ì¡°ì •
    config.epochs = start_epoch + remaining_epochs
    
    return train_model_improved(model, train_loader, val_loader, config)


if __name__ == "__main__":
    print("ê°œì„ ëœ SAM-Med3D í›ˆë ¨ ëª¨ë“ˆ")
    print("ì´ íŒŒì¼ì€ ì§ì ‘ ì‹¤í–‰ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. train.pyì—ì„œ importí•˜ì—¬ ì‚¬ìš©í•˜ì„¸ìš”.")
"""
ê°œì„ ëœ SAM-Med3D í›ˆë ¨ ëª¨ë“ˆ
- Mixed precision training ì§€ì›
- Gradient accumulation
- í–¥ìƒëœ ë©”ëª¨ë¦¬ ê´€ë¦¬
- ì•ˆì •ì ì¸ Loss ê³„ì‚°
- ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
"""

import torch
import torch.nn.functional as F
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm
import os
import time
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.patches as mpatches
from matplotlib.colors import ListedColormap
import wandb
from pathlib import Path
import gc


def calculate_dice_safe(pred, target, smooth=1e-6):
    """ì•ˆì „í•œ Dice score ê³„ì‚°"""
    try:
        pred = torch.sigmoid(pred)
        pred_flat = pred.view(pred.size(0), -1)
        target_flat = target.view(target.size(0), -1)
        
        intersection = (pred_flat * target_flat).sum(dim=1)
        dice = (2. * intersection + smooth) / (pred_flat.sum(dim=1) + target_flat.sum(dim=1) + smooth)
        return dice.mean().item()
    except Exception as e:
        print(f"âš ï¸ Dice ê³„ì‚° ì˜¤ë¥˜: {e}")
        return 0.0


def calculate_iou_safe(pred, target, smooth=1e-6):
    """ì•ˆì „í•œ IoU score ê³„ì‚°"""
    try:
        pred = torch.sigmoid(pred)
        pred_flat = pred.view(pred.size(0), -1)
        target_flat = target.view(target.size(0), -1)
        
        intersection = (pred_flat * target_flat).sum(dim=1)
        union = pred_flat.sum(dim=1) + target_flat.sum(dim=1) - intersection
        iou = (intersection + smooth) / (union + smooth)
        return iou.mean().item()
    except Exception as e:
        print(f"âš ï¸ IoU ê³„ì‚° ì˜¤ë¥˜: {e}")
        return 0.0


def safe_tensor_operations(pred_masks, target_masks):
    """í…ì„œ ì—°ì‚°ì„ ìœ„í•œ ì•ˆì „í•œ ì „ì²˜ë¦¬"""
    if not pred_masks.is_contiguous():
        pred_masks = pred_masks.contiguous()
    if not target_masks.is_contiguous():
        target_masks = target_masks.contiguous()
    
    return pred_masks, target_masks


def create_visualization(image, target_mask, pred_mask, patient_id, slice_idx=None):
    """ë¶„í•  ê²°ê³¼ ì‹œê°í™” ìƒì„± - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë²„ì „"""
    try:
        # ì…ë ¥ ë°ì´í„° ì²˜ë¦¬
        if isinstance(image, torch.Tensor):
            image = image.detach().cpu().numpy()
        if isinstance(target_mask, torch.Tensor):
            target_mask = target_mask.detach().cpu().numpy()
        if isinstance(pred_mask, torch.Tensor):
            pred_mask = torch.sigmoid(pred_mask).detach().cpu().numpy()
        
        # 3D ë°ì´í„°ì—ì„œ ì¤‘ì‹¬ ìŠ¬ë¼ì´ìŠ¤ ì„ íƒ
        if len(image.shape) == 4:  # (C, D, H, W)
            image = image[0]  # ì²« ë²ˆì§¸ ì±„ë„
            target_mask = target_mask[0]
            pred_mask = pred_mask[0]
        
        if slice_idx is None:
            slice_idx = image.shape[0] // 2  # ì¤‘ì‹¬ ìŠ¬ë¼ì´ìŠ¤
        
        img_slice = image[slice_idx]
        target_slice = target_mask[slice_idx]
        pred_slice = pred_mask[slice_idx]
        
        # ì˜ˆì¸¡ì„ ì´ì§„í™”
        pred_binary = (pred_slice > 0.5).astype(np.float32)
        
        # ì‹œê°í™” ìƒì„±
        fig, axes = plt.subplots(1, 4, figsize=(16, 4))
        
        # 1. ì›ë³¸ ì´ë¯¸ì§€
        axes[0].imshow(img_slice, cmap='gray', aspect='equal')
        axes[0].set_title(f'Original\n{patient_id}')
        axes[0].axis('off')
        
        # 2. Ground Truth
        axes[1].imshow(img_slice, cmap='gray', aspect='equal')
        if target_slice.max() > 0:
            axes[1].imshow(target_slice, cmap='Reds', alpha=0.6, aspect='equal')
            axes[1].contour(target_slice, levels=[0.5], colors='red', linewidths=2)
        axes[1].set_title(f'Ground Truth\n({target_slice.sum():.0f} pixels)')
        axes[1].axis('off')
        
        # 3. Prediction
        axes[2].imshow(img_slice, cmap='gray', aspect='equal')
        if pred_binary.max() > 0:
            axes[2].imshow(pred_binary, cmap='Blues', alpha=0.6, aspect='equal')
            axes[2].contour(pred_binary, levels=[0.5], colors='blue', linewidths=2)
        axes[2].set_title(f'Prediction\n({pred_binary.sum():.0f} pixels)')
        axes[2].axis('off')
        
        # 4. Overlay Comparison
        axes[3].imshow(img_slice, cmap='gray', aspect='equal')
        if target_slice.max() > 0:
            axes[3].contour(target_slice, levels=[0.5], colors='red', linewidths=2, label='GT')
        if pred_binary.max() > 0:
            axes[3].contour(pred_binary, levels=[0.5], colors='blue', linewidths=2, label='Pred')
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        dice = calculate_dice_safe(torch.tensor(pred_slice), torch.tensor(target_slice))
        iou = calculate_iou_safe(torch.tensor(pred_slice), torch.tensor(target_slice))
        
        axes[3].set_title(f'Comparison\nDice: {dice:.3f}, IoU: {iou:.3f}')
        axes[3].axis('off')
        
        # ë²”ë¡€ ì¶”ê°€
        red_patch = mpatches.Patch(color='red', label='Ground Truth')
        blue_patch = mpatches.Patch(color='blue', label='Prediction')
        axes[3].legend(handles=[red_patch, blue_patch], loc='upper right')
        
        plt.tight_layout()
        return fig
        
    except Exception as e:
        print(f"âš ï¸ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
        return None


def log_sample_predictions(model, dataloader, device, config, epoch, phase="val", max_samples=2):
    """ìƒ˜í”Œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ WandBì— ë¡œê¹… - ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë²„ì „"""
    if not config.use_wandb or not config.log_images:
        return
    
    model.eval()
    logged_samples = 0
    
    with torch.no_grad():
        for batch_idx, batch in enumerate(dataloader):
            if batch is None or batch["patient_id"][0] == "dummy":
                continue
            
            if logged_samples >= max_samples:
                break
            
            try:
                image = batch["image"].to(device, non_blocking=True)
                mask = batch["mask"].to(device, non_blocking=True)
                patient_id = batch["patient_id"][0]
                
                # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ìœ„í•´ ì²« ë²ˆì§¸ ìƒ˜í”Œë§Œ ì²˜ë¦¬
                if image.size(0) > 1:
                    image = image[:1]
                    mask = mask[:1]
                    patient_id = patient_id if isinstance(patient_id, str) else patient_id[0]
                
                # í…ì„œ ì—°ì†ì„± ë³´ì¥
                image = image.contiguous()
                mask = mask.contiguous()
                
                # ì˜ˆì¸¡
                with autocast(enabled=config.use_mixed_precision):
                    results = model(image, mask)
                    pred_masks = results['pred_masks']
                
                # ì‹œê°í™” ìƒì„±
                fig = create_visualization(
                    image[0], mask[0], pred_masks[0], patient_id
                )
                
                if fig is not None:
                    # WandBì— ë¡œê¹…
                    wandb.log({
                        f"{phase}_predictions/sample_{logged_samples+1}": wandb.Image(
                            fig, 
                            caption=f"Epoch {epoch} - {patient_id}"
                        )
                    }, step=epoch)
                    
                    plt.close(fig)
                    logged_samples += 1
                
                # ë©”ëª¨ë¦¬ ì •ë¦¬
                del image, mask, results
                torch.cuda.empty_cache()
                
            except Exception as e:
                print(f"âš ï¸ ì´ë¯¸ì§€ ë¡œê¹… ì‹¤íŒ¨ (ë°°ì¹˜ {batch_idx}): {e}")
                continue
            
            # ë©”ëª¨ë¦¬ ë³´í˜¸ë¥¼ ìœ„í•œ ì¡°ê¸° ì¢…ë£Œ
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1e9
                if memory_used > 10:  # 10GB ì´ìƒ ì‚¬ìš©ì‹œ ì¤‘ë‹¨
                    print(f"âš ï¸ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼ ({memory_used:.1f}GB), ë¡œê¹… ì¤‘ë‹¨")
                    break


def log_metrics_to_wandb(metrics_dict, epoch, phase="train"):
    """ë©”íŠ¸ë¦­ì„ WandBì— ë¡œê¹…"""
    if wandb.run is None:
        return
    
    try:
        log_dict = {}
        for key, value in metrics_dict.items():
            if isinstance(value, (int, float)) and not np.isnan(value):
                log_dict[f"{phase}_{key}"] = value
        
        if log_dict:  # ìœ íš¨í•œ ë©”íŠ¸ë¦­ì´ ìˆì„ ë•Œë§Œ ë¡œê¹…
            wandb.log(log_dict, step=epoch)
    except Exception as e:
        print(f"âš ï¸ WandB ë©”íŠ¸ë¦­ ë¡œê¹… ì‹¤íŒ¨: {e}")


def get_memory_info():
    """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì •ë³´ ë°˜í™˜"""
    if not torch.cuda.is_available():
        return {"available": False}
    
    return {
        "available": True,
        "allocated_gb": torch.cuda.memory_allocated() / 1e9,
        "cached_gb": torch.cuda.memory_reserved() / 1e9,
        "max_allocated_gb": torch.cuda.max_memory_allocated() / 1e9
    }


def cleanup_memory():
    """ë©”ëª¨ë¦¬ ì •ë¦¬"""
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()


class EarlyStopping:
    """ê°œì„ ëœ Early Stopping í´ë˜ìŠ¤"""
    
    def __init__(self, patience=7, min_delta=0.001, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_score = None
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, score, model):
        if self.best_score is None:
            self.best_score = score
            self.save_checkpoint(model)
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                if self.restore_best_weights and self.best_weights is not None:
                    model.load_state_dict(self.best_weights)
                return True
        else:
            self.best_score = score
            self.counter = 0
            self.save_checkpoint(model)
        return False
    
    def save_checkpoint(self, model):
        if self.restore_best_weights:
            self.best_weights = model.state_dict().copy()


def create_optimizer(model, config):
    """ì˜µí‹°ë§ˆì´ì € ìƒì„±"""
    # íŒŒë¼ë¯¸í„° ê·¸ë£¹ ë¶„ë¦¬ (ë‹¤ë¥¸ í•™ìŠµë¥  ì ìš© ê°€ëŠ¥)
    param_groups = [
        {
            'params': [p for n, p in model.named_parameters() 
                      if 'repr_head' in n and p.requires_grad],
            'lr': config.learning_rate * 2,  # representation headëŠ” ë” ë†’ì€ í•™ìŠµë¥ 
            'weight_decay': config.weight_decay
        },
        {
            'params': [p for n, p in model.named_parameters() 
                      if 'repr_head' not in n and p.requires_grad],
            'lr': config.learning_rate,
            'weight_decay': config.weight_decay
        }
    ]
    
    # ë¹ˆ ê·¸ë£¹ ì œê±°
    param_groups = [group for group in param_groups if len(group['params']) > 0]
    
    if not param_groups:
        raise ValueError("í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    optimizer = torch.optim.AdamW(
        param_groups,
        lr=config.learning_rate,
        weight_decay=config.weight_decay,
        eps=1e-8,
        betas=(0.9, 0.999)
    )
    
    return optimizer


def create_scheduler(optimizer, config):
    """í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±"""
    if config.scheduler_type == "reduce_on_plateau":
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer,
            mode='max',
            factor=config.scheduler_factor,
            patience=config.scheduler_patience,
            min_lr=config.min_lr,
            verbose=True
        )
    elif config.scheduler_type == "cosine":
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer,
            T_max=config.epochs,
            eta_min=config.min_lr
        )
    elif config.scheduler_type == "step":
        scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer,
            step_size=config.epochs // 4,
            gamma=config.scheduler_factor
        )
    else:
        scheduler = None
    
    return scheduler


def print_training_progress(epoch, total_epochs, metrics, elapsed_time, memory_info):
    """í›ˆë ¨ ì§„í–‰ ìƒí™© ì¶œë ¥"""
    print(f"\nğŸ“Š Epoch {epoch+1}/{total_epochs} ì™„ë£Œ")
    print(f"â±ï¸ ì†Œìš” ì‹œê°„: {elapsed_time:.1f}ì´ˆ")
    
    # ë©”íŠ¸ë¦­ ì¶œë ¥
    if 'train' in metrics:
        train_metrics = metrics['train']
        print(f"ğŸš‚ Train - Loss: {train_metrics.get('loss', 0):.4f}, "
              f"Dice: {train_metrics.get('dice', 0):.4f}, "
              f"IoU: {train_metrics.get('iou', 0):.4f}")
    
    if 'val' in metrics:
        val_metrics = metrics['val']
        print(f"ğŸ” Val   - Loss: {val_metrics.get('loss', 0):.4f}, "
              f"Dice: {val_metrics.get('dice', 0):.4f}, "
              f"IoU: {val_metrics.get('iou', 0):.4f}")
    
    # ë©”ëª¨ë¦¬ ì •ë³´
    if memory_info.get("available", False):
        print(f"ğŸ–¥ï¸ GPU ë©”ëª¨ë¦¬: {memory_info['allocated_gb']:.1f}GB allocated, "
              f"{memory_info['cached_gb']:.1f}GB cached")
import os
import random
import numpy as np
import torch
import torch.nn as nn
import wandb
import matplotlib.pyplot as plt
import pickle
import json
import time
from typing import Dict, List, Optional, Tuple, Any, Union
import pandas as pd
from pathlib import Path
from datetime import datetime
import psutil  # ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ìš©


def set_seed(seed: int = 42):
    """ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    print(f"ğŸ² Seed set to {seed}")


def setup_wandb(config, project_name: str = "sam-med3d-finetuning"):
    """WandB ì´ˆê¸°í™”"""
    try:
        wandb.init(
            project=project_name,
            name=config.get_experiment_name(),
            config={
                # Training config
                'epochs': config.epochs,
                'batch_size': config.batch_size,
                'learning_rate': config.learning_rate,
                'weight_decay': config.optimizer_config['weight_decay'],
                
                # Model config
                'input_size': config.input_size,
                'representation_dim': config.representation_dim,
                'freeze_prompt_encoder': config.freeze_prompt_encoder,
                'use_gradient_checkpointing': config.use_gradient_checkpointing,
                
                # Loss weights
                **{f'loss_weight_{k}': v for k, v in config.loss_weights.items()},
                
                # Other settings
                'patience': config.patience,
                'gradient_accumulation_steps': config.gradient_accumulation_steps,
            },
            tags=['sam-med3d', 'brain-ct', 'segmentation', 'fine-tuning']
        )
        
        print(f"ğŸ”— WandB initialized: {wandb.run.url}")
        
    except Exception as e:
        print(f"âŒ WandB initialization failed: {e}")
        print("Continuing without WandB logging...")


def save_model_checkpoint(model, optimizer, scheduler, epoch, metrics, config, 
                         checkpoint_type: str = "regular"):
    """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    
    checkpoint_data = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'metrics': metrics,
        'config': config,
        'checkpoint_type': checkpoint_type
    }
    
    # íŒŒì¼ëª… ì„¤ì •
    if checkpoint_type == "best":
        filename = "best_model.pth"
    elif checkpoint_type == "latest":
        filename = "latest_model.pth"
    elif checkpoint_type == "blip2_encoder":
        filename = "blip2_compatible_encoder.pth"
    else:
        filename = f"checkpoint_epoch_{epoch}.pth"
    
    filepath = os.path.join(config.output_dir, filename)
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(config.output_dir, exist_ok=True)
    
    try:
        torch.save(checkpoint_data, filepath)
        print(f"ğŸ’¾ Checkpoint saved: {filepath}")
        return filepath
    except Exception as e:
        print(f"âŒ Failed to save checkpoint: {e}")
        return None


def load_model_checkpoint(model, checkpoint_path, optimizer=None, scheduler=None, 
                         load_optimizer: bool = True):
    """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    print(f"ğŸ“‚ Loading checkpoint: {checkpoint_path}")
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # ëª¨ë¸ weights ë¡œë“œ
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # Optimizer ë¡œë“œ (ì„ íƒì )
        if load_optimizer and optimizer is not None and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # Scheduler ë¡œë“œ (ì„ íƒì )
        if load_optimizer and scheduler is not None and 'scheduler_state_dict' in checkpoint:
            if checkpoint['scheduler_state_dict'] is not None:
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        # ë©”íƒ€ë°ì´í„° ë°˜í™˜
        metadata = {
            'epoch': checkpoint.get('epoch', 0),
            'metrics': checkpoint.get('metrics', {}),
            'config': checkpoint.get('config', None)
        }
        
        print(f"âœ… Checkpoint loaded successfully")
        if 'epoch' in checkpoint:
            print(f"   ğŸ“Š Epoch: {checkpoint['epoch']}")
        if 'metrics' in checkpoint:
            metrics = checkpoint['metrics']
            if isinstance(metrics, dict):
                for key, value in metrics.items():
                    if isinstance(value, (int, float)):
                        print(f"   ğŸ“Š {key}: {value:.4f}")
        
        return metadata
        
    except Exception as e:
        print(f"âŒ Failed to load checkpoint: {e}")
        raise


def visualize_predictions(images, masks, predictions, save_path: str = None, 
                         max_samples: int = 4, slice_idx: int = None):
    """ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""
    
    batch_size = min(images.shape[0], max_samples)
    
    # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
    if torch.is_tensor(images):
        images = images.cpu().numpy()
    if torch.is_tensor(masks):
        masks = masks.cpu().numpy()
    if torch.is_tensor(predictions):
        predictions = torch.sigmoid(predictions).cpu().numpy()
    
    # 3D ë°ì´í„°ì˜ ê²½ìš° ì¤‘ê°„ slice ì„ íƒ
    if slice_idx is None:
        slice_idx = images.shape[2] // 2  # ì¤‘ê°„ slice
    
    fig, axes = plt.subplots(batch_size, 3, figsize=(12, 4 * batch_size))
    if batch_size == 1:
        axes = axes.reshape(1, -1)
    
    for i in range(batch_size):
        # ì´ë¯¸ì§€
        img_slice = images[i, 0, slice_idx]  # [H, W]
        axes[i, 0].imshow(img_slice, cmap='gray')
        axes[i, 0].set_title(f'Sample {i+1}: Original')
        axes[i, 0].axis('off')
        
        # Ground truth ë§ˆìŠ¤í¬
        mask_slice = masks[i, 0, slice_idx]
        axes[i, 1].imshow(img_slice, cmap='gray', alpha=0.7)
        axes[i, 1].imshow(mask_slice, cmap='Reds', alpha=0.5)
        axes[i, 1].set_title(f'Sample {i+1}: Ground Truth')
        axes[i, 1].axis('off')
        
        # ì˜ˆì¸¡ ë§ˆìŠ¤í¬
        pred_slice = predictions[i, 0, slice_idx]
        axes[i, 2].imshow(img_slice, cmap='gray', alpha=0.7)
        axes[i, 2].imshow(pred_slice, cmap='Blues', alpha=0.5)
        axes[i, 2].set_title(f'Sample {i+1}: Prediction')
        axes[i, 2].axis('off')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"ğŸ“Š Visualization saved: {save_path}")
    
    plt.show()
    return fig


def calculate_dataset_stats(csv_path: str, image_col: str = "path", 
                           mask_col: str = "mask", num_samples: int = 100):
    """ë°ì´í„°ì…‹ í†µê³„ ê³„ì‚°"""
    
    print(f"ğŸ“Š Calculating dataset statistics: {csv_path}")
    
    df = pd.read_csv(csv_path)
    total_samples = len(df)
    
    # ìƒ˜í”Œë§
    sample_size = min(num_samples, total_samples)
    sample_indices = np.random.choice(total_samples, sample_size, replace=False)
    
    image_intensities = []
    mask_ratios = []
    image_shapes = []
    
    valid_samples = 0
    
    for idx in sample_indices:
        try:
            row = df.iloc[idx]
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            image_path = row[image_col]
            mask_path = row[mask_col]
            
            with open(image_path, 'rb') as f:
                image = pickle.load(f)
            with open(mask_path, 'rb') as f:
                mask = pickle.load(f)
            
            # numpy ë³€í™˜
            if not isinstance(image, np.ndarray):
                image = np.array(image)
            if not isinstance(mask, np.ndarray):
                mask = np.array(mask)
            
            # í†µê³„ ê³„ì‚°
            image_intensities.extend(image.flatten())
            mask_ratios.append(np.mean(mask > 0))
            image_shapes.append(image.shape)
            valid_samples += 1
            
        except Exception as e:
            print(f"Error processing sample {idx}: {e}")
            continue
    
    if valid_samples == 0:
        print("âŒ No valid samples found")
        return None
    
    # ê²°ê³¼ ì •ë¦¬
    stats = {
        'total_samples': total_samples,
        'analyzed_samples': valid_samples,
        'image_intensity': {
            'mean': np.mean(image_intensities),
            'std': np.std(image_intensities),
            'min': np.min(image_intensities),
            'max': np.max(image_intensities),
            'percentiles': {
                '1%': np.percentile(image_intensities, 1),
                '5%': np.percentile(image_intensities, 5),
                '95%': np.percentile(image_intensities, 95),
                '99%': np.percentile(image_intensities, 99)
            }
        },
        'mask_statistics': {
            'mean_positive_ratio': np.mean(mask_ratios),
            'std_positive_ratio': np.std(mask_ratios),
            'min_positive_ratio': np.min(mask_ratios),
            'max_positive_ratio': np.max(mask_ratios)
        },
        'image_shapes': {
            'unique_shapes': list(set(image_shapes)),
            'most_common_shape': max(set(image_shapes), key=image_shapes.count)
        }
    }
    
    # í†µê³„ ì¶œë ¥
    print(f"ğŸ“Š Dataset Statistics (based on {valid_samples} samples):")
    print(f"   ğŸ“ˆ Image Intensity:")
    print(f"      Mean: {stats['image_intensity']['mean']:.3f} Â± {stats['image_intensity']['std']:.3f}")
    print(f"      Range: [{stats['image_intensity']['min']:.3f}, {stats['image_intensity']['max']:.3f}]")
    
    print(f"   ğŸ¯ Mask Statistics:")
    print(f"      Positive ratio: {stats['mask_statistics']['mean_positive_ratio']:.3%} Â± {stats['mask_statistics']['std_positive_ratio']:.3%}")
    print(f"      Range: [{stats['mask_statistics']['min_positive_ratio']:.3%}, {stats['mask_statistics']['max_positive_ratio']:.3%}]")
    
    print(f"   ğŸ“ Image Shapes:")
    print(f"      Most common: {stats['image_shapes']['most_common_shape']}")
    print(f"      Unique shapes: {len(stats['image_shapes']['unique_shapes'])}")
    
    return stats


def create_output_directories(config):
    """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
    dirs_to_create = [
        config.output_dir,
        os.path.join(config.output_dir, 'checkpoints'),
        os.path.join(config.output_dir, 'visualizations'),
        os.path.join(config.output_dir, 'logs')
    ]
    
    for dir_path in dirs_to_create:
        os.makedirs(dir_path, exist_ok=True)
        print(f"ğŸ“ Directory created: {dir_path}")


def log_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        cached = torch.cuda.memory_reserved() / 1e9
        total = torch.cuda.get_device_properties(0).total_memory / 1e9
        
        print(f"ğŸ–¥ï¸ GPU Memory: {allocated:.1f}GB allocated, {cached:.1f}GB cached, {total:.1f}GB total")
        
        return {
            'allocated_gb': allocated,
            'cached_gb': cached,
            'total_gb': total,
            'utilization': allocated / total
        }
    else:
        print("ğŸ–¥ï¸ GPU not available")
        return None


def validate_data_paths(config):
    """ê°œì„ ëœ ë°ì´í„° ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬"""
    errors = []
    warnings = []
    
    # CSV íŒŒì¼ í™•ì¸ (ì†ì„±ëª… ìˆ˜ì •)
    csv_paths = [
        ("Training", getattr(config, 'train_csv', None)),
        ("Validation", getattr(config, 'val_csv', None))
    ]
    
    for name, path in csv_paths:
        if not path:
            errors.append(f"{name} CSV path is empty")
            continue
            
        if not os.path.exists(path):
            errors.append(f"{name} CSV file not found: {path}")
            continue
        
        try:
            df = pd.read_csv(path)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_cols = ["path"]
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                errors.append(f"{name} CSV missing columns: {missing_cols}")
            
            # ICH ì»¬ëŸ¼ í™•ì¸ (ì„ íƒì )
            if 'ich' in df.columns:
                ich_count = len(df[df['ich'] == 1])
                print(f"âœ… {name} CSV: {len(df)} total, {ich_count} ICH samples")
                if ich_count == 0:
                    warnings.append(f"{name} CSV has no ICH samples")
            else:
                print(f"âœ… {name} CSV: {len(df)} samples")
            
            # ìƒ˜í”Œ ìˆ˜ í™•ì¸
            if len(df) == 0:
                errors.append(f"{name} CSV is empty")
            elif len(df) < 5:
                warnings.append(f"{name} CSV has only {len(df)} samples")
            
            # ì‹¤ì œ íŒŒì¼ ì¡´ì¬ í™•ì¸ (ì¼ë¶€ ìƒ˜í”Œ)
            sample_size = min(5, len(df))
            missing_files = 0
            for idx in range(sample_size):
                file_path = df.iloc[idx]['path']
                if not os.path.exists(file_path):
                    missing_files += 1
            
            if missing_files > 0:
                warnings.append(f"{name}: {missing_files}/{sample_size} sample files not found")
                
        except Exception as e:
            errors.append(f"Error reading {name} CSV: {e}")
    
    # ë§ˆìŠ¤í¬ ë””ë ‰í† ë¦¬ í™•ì¸
    mask_dir = getattr(config, 'mask_dir', None)
    if mask_dir:
        if not os.path.exists(mask_dir):
            errors.append(f"Mask directory not found: {mask_dir}")
        else:
            # ë§ˆìŠ¤í¬ íŒŒì¼ ê°œìˆ˜ í™•ì¸
            mask_files = list(Path(mask_dir).glob("*.nii"))
            print(f"âœ… Mask directory: {len(mask_files)} .nii files found")
            if len(mask_files) == 0:
                warnings.append("No .nii mask files found in mask directory")
    else:
        warnings.append("Mask directory not specified")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
    try:
        os.makedirs(config.output_dir, exist_ok=True)
        test_file = os.path.join(config.output_dir, "test_write.tmp")
        with open(test_file, 'w') as f:
            f.write("test")
        os.remove(test_file)
        print(f"âœ… Output directory writable: {config.output_dir}")
    except Exception as e:
        errors.append(f"Output directory not writable: {e}")
    
    # ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
    try:
        disk_usage = psutil.disk_usage(config.output_dir)
        free_gb = disk_usage.free / (1024**3)
        if free_gb < 10:  # 10GB ë¯¸ë§Œ
            warnings.append(f"Low disk space: {free_gb:.1f}GB available")
        else:
            print(f"âœ… Disk space: {free_gb:.1f}GB available")
    except Exception as e:
        warnings.append(f"Could not check disk space: {e}")
    
    # ê²°ê³¼ ë°˜í™˜
    if errors:
        print("âŒ Data validation errors:")
        for error in errors:
            print(f"   - {error}")
        return False
    
    if warnings:
        print("âš ï¸ Data validation warnings:")
        for warning in warnings:
            print(f"   - {warning}")
    
    print("âœ… Data paths validation passed")
    return True


def validate_system_requirements():
    """ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ê²€ì‚¬"""
    issues = []
    
    # GPU í™•ì¸
    if not torch.cuda.is_available():
        issues.append("CUDA not available - training will be very slow")
    else:
        gpu_count = torch.cuda.device_count()
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        print(f"âœ… GPU: {gpu_count} device(s), {gpu_memory:.1f}GB memory")
        
        if gpu_memory < 8:
            issues.append(f"GPU memory ({gpu_memory:.1f}GB) may be insufficient for 3D training")
    
    # RAM í™•ì¸
    ram_gb = psutil.virtual_memory().total / (1024**3)
    if ram_gb < 16:
        issues.append(f"System RAM ({ram_gb:.1f}GB) may be insufficient")
    else:
        print(f"âœ… System RAM: {ram_gb:.1f}GB")
    
    # PyTorch ë²„ì „ í™•ì¸
    torch_version = torch.__version__
    print(f"âœ… PyTorch version: {torch_version}")
    
    # í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸
    try:
        import monai
        print(f"âœ… MONAI version: {monai.__version__}")
    except ImportError:
        issues.append("MONAI not installed")
    
    try:
        import nibabel
        print(f"âœ… nibabel version: {nibabel.__version__}")
    except ImportError:
        issues.append("nibabel not installed")
    
    if issues:
        print("âš ï¸ System requirement issues:")
        for issue in issues:
            print(f"   - {issue}")
        return False
    
    print("âœ… System requirements check passed")
    return True


def format_time(seconds: float) -> str:
    """ì‹œê°„ì„ ê°€ë…ì„± ì¢‹ì€ í˜•íƒœë¡œ í¬ë§·"""
    if seconds < 60:
        return f"{seconds:.1f}s"
    elif seconds < 3600:
        return f"{seconds/60:.1f}m"
    else:
        return f"{seconds/3600:.1f}h"


def get_model_size(model) -> Dict[str, Any]:
    """ëª¨ë¸ í¬ê¸° ì •ë³´ ê³„ì‚°"""
    param_count = sum(p.numel() for p in model.parameters())
    trainable_param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (bytes)
    param_size = sum(p.numel() * p.element_size() for p in model.parameters())
    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
    
    total_size_mb = (param_size + buffer_size) / 1e6
    
    return {
        'total_params': param_count,
        'trainable_params': trainable_param_count,
        'frozen_params': param_count - trainable_param_count,
        'size_mb': total_size_mb,
        'trainable_ratio': trainable_param_count / param_count if param_count > 0 else 0
    }


def print_model_info(model, config):
    """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
    model_info = get_model_size(model)
    
    print("ğŸ§  Model Information:")
    print(f"   ğŸ“Š Total parameters: {model_info['total_params']:,}")
    print(f"   ğŸ”§ Trainable parameters: {model_info['trainable_params']:,}")
    print(f"   â„ï¸ Frozen parameters: {model_info['frozen_params']:,}")
    print(f"   ğŸ“ Model size: {model_info['size_mb']:.1f} MB")
    print(f"   ğŸ”„ Trainable ratio: {model_info['trainable_ratio']:.1%}")
    
    # ëª¨ë“ˆë³„ íŒŒë¼ë¯¸í„° ë¶„ì„
    print("\nğŸ” Module-wise parameter count:")
    for name, module in model.named_children():
        module_params = sum(p.numel() for p in module.parameters())
        trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)
        
        if module_params > 0:
            print(f"   {name}: {module_params:,} ({trainable_params:,} trainable)")


def create_training_summary(config, results):
    """í›ˆë ¨ ê²°ê³¼ ìš”ì•½ ìƒì„±"""
    summary = {
        "training_config": {
            "model": "SAM-Med3D",
            "task": "Brain CT ICH Segmentation",
            "input_size": config.input_size,
            "batch_size": config.batch_size,
            "learning_rate": config.learning_rate,
            "epochs": config.epochs,
            "optimizer": "AdamW",
            "scheduler": getattr(config, 'scheduler_type', 'reduce_on_plateau'),
            "mixed_precision": getattr(config, 'use_mixed_precision', False),
            "gradient_accumulation": getattr(config, 'gradient_accumulation_steps', 1)
        },
        "training_results": results.get('summary', {}),
        "best_metrics": results.get('best_metrics', {}),
        "timestamp": datetime.now().isoformat(),
        "system_info": {
            "pytorch_version": torch.__version__,
            "cuda_available": torch.cuda.is_available(),
            "gpu_count": torch.cuda.device_count() if torch.cuda.is_available() else 0,
            "platform": os.name
        }
    }
    
    return summary


def save_training_summary(config, results, filepath=None):
    """í›ˆë ¨ ìš”ì•½ì„ JSON íŒŒì¼ë¡œ ì €ì¥"""
    if filepath is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filepath = Path(config.output_dir) / f"training_summary_{timestamp}.json"
    
    summary = create_training_summary(config, results)
    
    try:
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        print(f"ğŸ“Š í›ˆë ¨ ìš”ì•½ ì €ì¥: {filepath}")
        return filepath
    except Exception as e:
        print(f"âŒ í›ˆë ¨ ìš”ì•½ ì €ì¥ ì‹¤íŒ¨: {e}")
        return None


def estimate_training_time(config, sample_batch_time=None):
    """í›ˆë ¨ ì‹œê°„ ì¶”ì •"""
    if sample_batch_time is None:
        # ë°°ì¹˜ í¬ê¸°ì™€ ëª¨ë¸ ë³µì¡ë„ ê¸°ë°˜ ì¶”ì •
        base_time = 2.0  # ê¸°ë³¸ ë°°ì¹˜ ì‹œê°„ (ì´ˆ)
        size_factor = (config.input_size[0] * config.input_size[1] * config.input_size[2]) / (128**3)
        batch_factor = config.batch_size / 2  # ê¸°ì¤€ ë°°ì¹˜ í¬ê¸° 2
        
        estimated_batch_time = base_time * size_factor * batch_factor
    else:
        estimated_batch_time = sample_batch_time
    
    # ë°ì´í„°ë¡œë” í¬ê¸° ì¶”ì • (ê°€ëŠ¥í•œ ê²½ìš°)
    try:
        train_df = pd.read_csv(config.train_csv)
        if 'ich' in train_df.columns:
            train_samples = len(train_df[train_df['ich'] == 1])
        else:
            train_samples = len(train_df)
        
        batches_per_epoch = train_samples // config.batch_size
        total_batches = batches_per_epoch * config.epochs
        
        estimated_hours = (total_batches * estimated_batch_time) / 3600
        
        print(f"â±ï¸ í›ˆë ¨ ì‹œê°„ ì¶”ì •:")
        print(f"   í›ˆë ¨ ìƒ˜í”Œ: {train_samples}")
        print(f"   ì—í¬í¬ë‹¹ ë°°ì¹˜: {batches_per_epoch}")
        print(f"   ì´ ë°°ì¹˜: {total_batches:,}")
        print(f"   ë°°ì¹˜ë‹¹ ì‹œê°„: {estimated_batch_time:.1f}ì´ˆ")
        print(f"   ì˜ˆìƒ ì´ ì‹œê°„: {estimated_hours:.1f}ì‹œê°„ ({estimated_hours/24:.1f}ì¼)")
        
        return {
            'estimated_hours': estimated_hours,
            'estimated_days': estimated_hours / 24,
            'batches_per_epoch': batches_per_epoch,
            'total_batches': total_batches,
            'batch_time_seconds': estimated_batch_time
        }
        
    except Exception as e:
        print(f"âš ï¸ í›ˆë ¨ ì‹œê°„ ì¶”ì • ì‹¤íŒ¨: {e}")
        return None


def monitor_training_progress(epoch, total_epochs, metrics, start_time):
    """í›ˆë ¨ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§"""
    elapsed_time = time.time() - start_time
    progress = (epoch + 1) / total_epochs
    
    # ë‚¨ì€ ì‹œê°„ ì¶”ì •
    if progress > 0:
        estimated_total_time = elapsed_time / progress
        remaining_time = estimated_total_time - elapsed_time
    else:
        remaining_time = 0
    
    # í˜„ì¬ ì„±ëŠ¥
    current_performance = {
        'train_dice': metrics.get('train', {}).get('dice', 0),
        'val_dice': metrics.get('val', {}).get('dice', 0),
        'train_loss': metrics.get('train', {}).get('loss', 0),
        'val_loss': metrics.get('val', {}).get('loss', 0)
    }
    
    # ì§„í–‰ ìƒí™© ì¶œë ¥
    print(f"\nğŸ“ˆ ì§„í–‰ ìƒí™©:")
    print(f"   ì—í¬í¬: {epoch+1}/{total_epochs} ({progress:.1%})")
    print(f"   ê²½ê³¼ ì‹œê°„: {format_time(elapsed_time)}")
    print(f"   ë‚¨ì€ ì‹œê°„: {format_time(remaining_time)}")
    print(f"   í˜„ì¬ ì„±ëŠ¥: Train Dice {current_performance['train_dice']:.3f}, Val Dice {current_performance['val_dice']:.3f}")
    
    return {
        'progress': progress,
        'elapsed_time': elapsed_time,
        'remaining_time': remaining_time,
        'performance': current_performance
    }


def cleanup_old_checkpoints(output_dir, keep_last_n=3):
    """ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬"""
    try:
        checkpoint_pattern = Path(output_dir) / "checkpoint_epoch_*.pth"
        checkpoints = list(Path(output_dir).glob("checkpoint_epoch_*.pth"))
        
        if len(checkpoints) <= keep_last_n:
            return
        
        # ì—í¬í¬ ë²ˆí˜¸ ê¸°ì¤€ ì •ë ¬
        checkpoints.sort(key=lambda x: int(x.stem.split('_')[-1]))
        
        # ì˜¤ë˜ëœ ê²ƒë“¤ ì‚­ì œ
        for checkpoint in checkpoints[:-keep_last_n]:
            try:
                checkpoint.unlink()
                print(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ: {checkpoint.name}")
            except Exception as e:
                print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ ì‹¤íŒ¨: {checkpoint.name} - {e}")
                
    except Exception as e:
        print(f"âš ï¸ ì²´í¬í¬ì¸íŠ¸ ì •ë¦¬ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    print("Testing improved SAM utilities...")
    
    # ì‹œë“œ ì„¤ì • í…ŒìŠ¤íŠ¸
    set_seed(42)
    
    # ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ ê²€ì‚¬
    validate_system_requirements()
    
    # GPU ë©”ëª¨ë¦¬ í™•ì¸
    memory_info = log_gpu_memory()
    
    # ë”ë¯¸ ëª¨ë¸ë¡œ ëª¨ë¸ ì •ë³´ í…ŒìŠ¤íŠ¸
    class DummyModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv = nn.Conv3d(1, 64, 3, padding=1)
            self.fc = nn.Linear(64, 10)
            # ì¼ë¶€ íŒŒë¼ë¯¸í„° freeze
            for param in self.conv.parameters():
                param.requires_grad = False
    
    dummy_model = DummyModel()
    print_model_info(dummy_model, None)
    
    # ì‹œê°„ í¬ë§· í…ŒìŠ¤íŠ¸
    print(f"\nâ±ï¸ Time formatting test:")
    for seconds in [30, 90, 3660, 7200, 86400]:
        print(f"  {seconds}s -> {format_time(seconds)}")
    
    print("\nâœ… Improved SAM utilities tests completed!")
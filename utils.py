import os
import random
import numpy as np
import torch
import wandb
import matplotlib.pyplot as plt
import pickle
from typing import Dict, List, Optional, Tuple, Any
import pandas as pd
from pathlib import Path


def set_seed(seed: int = 42):
    """ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    print(f"ğŸ² Seed set to {seed}")


def setup_wandb(config, project_name: str = "sam-med3d-finetuning"):
    """WandB ì´ˆê¸°í™”"""
    try:
        wandb.init(
            project=project_name,
            name=config.get_experiment_name(),
            config={
                # Training config
                'epochs': config.epochs,
                'batch_size': config.batch_size,
                'learning_rate': config.learning_rate,
                'weight_decay': config.optimizer_config['weight_decay'],
                
                # Model config
                'input_size': config.input_size,
                'representation_dim': config.representation_dim,
                'freeze_prompt_encoder': config.freeze_prompt_encoder,
                'use_gradient_checkpointing': config.use_gradient_checkpointing,
                
                # Loss weights
                **{f'loss_weight_{k}': v for k, v in config.loss_weights.items()},
                
                # Other settings
                'patience': config.patience,
                'gradient_accumulation_steps': config.gradient_accumulation_steps,
            },
            tags=['sam-med3d', 'brain-ct', 'segmentation', 'fine-tuning']
        )
        
        print(f"ğŸ”— WandB initialized: {wandb.run.url}")
        
    except Exception as e:
        print(f"âŒ WandB initialization failed: {e}")
        print("Continuing without WandB logging...")


def save_model_checkpoint(model, optimizer, scheduler, epoch, metrics, config, 
                         checkpoint_type: str = "regular"):
    """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    
    checkpoint_data = {
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,
        'metrics': metrics,
        'config': config,
        'checkpoint_type': checkpoint_type
    }
    
    # íŒŒì¼ëª… ì„¤ì •
    if checkpoint_type == "best":
        filename = "best_model.pth"
    elif checkpoint_type == "latest":
        filename = "latest_model.pth"
    elif checkpoint_type == "blip2_encoder":
        filename = "blip2_compatible_encoder.pth"
    else:
        filename = f"checkpoint_epoch_{epoch}.pth"
    
    filepath = os.path.join(config.output_dir, filename)
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(config.output_dir, exist_ok=True)
    
    try:
        torch.save(checkpoint_data, filepath)
        print(f"ğŸ’¾ Checkpoint saved: {filepath}")
        return filepath
    except Exception as e:
        print(f"âŒ Failed to save checkpoint: {e}")
        return None


def load_model_checkpoint(model, checkpoint_path, optimizer=None, scheduler=None, 
                         load_optimizer: bool = True):
    """ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    
    if not os.path.exists(checkpoint_path):
        raise FileNotFoundError(f"Checkpoint not found: {checkpoint_path}")
    
    print(f"ğŸ“‚ Loading checkpoint: {checkpoint_path}")
    
    try:
        checkpoint = torch.load(checkpoint_path, map_location='cpu')
        
        # ëª¨ë¸ weights ë¡œë“œ
        model.load_state_dict(checkpoint['model_state_dict'])
        
        # Optimizer ë¡œë“œ (ì„ íƒì )
        if load_optimizer and optimizer is not None and 'optimizer_state_dict' in checkpoint:
            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # Scheduler ë¡œë“œ (ì„ íƒì )
        if load_optimizer and scheduler is not None and 'scheduler_state_dict' in checkpoint:
            if checkpoint['scheduler_state_dict'] is not None:
                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        # ë©”íƒ€ë°ì´í„° ë°˜í™˜
        metadata = {
            'epoch': checkpoint.get('epoch', 0),
            'metrics': checkpoint.get('metrics', {}),
            'config': checkpoint.get('config', None)
        }
        
        print(f"âœ… Checkpoint loaded successfully")
        if 'epoch' in checkpoint:
            print(f"   ğŸ“Š Epoch: {checkpoint['epoch']}")
        if 'metrics' in checkpoint:
            metrics = checkpoint['metrics']
            if isinstance(metrics, dict):
                for key, value in metrics.items():
                    if isinstance(value, (int, float)):
                        print(f"   ğŸ“Š {key}: {value:.4f}")
        
        return metadata
        
    except Exception as e:
        print(f"âŒ Failed to load checkpoint: {e}")
        raise


def visualize_predictions(images, masks, predictions, save_path: str = None, 
                         max_samples: int = 4, slice_idx: int = None):
    """ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""
    
    batch_size = min(images.shape[0], max_samples)
    
    # í…ì„œë¥¼ numpyë¡œ ë³€í™˜
    if torch.is_tensor(images):
        images = images.cpu().numpy()
    if torch.is_tensor(masks):
        masks = masks.cpu().numpy()
    if torch.is_tensor(predictions):
        predictions = torch.sigmoid(predictions).cpu().numpy()
    
    # 3D ë°ì´í„°ì˜ ê²½ìš° ì¤‘ê°„ slice ì„ íƒ
    if slice_idx is None:
        slice_idx = images.shape[2] // 2  # ì¤‘ê°„ slice
    
    fig, axes = plt.subplots(batch_size, 3, figsize=(12, 4 * batch_size))
    if batch_size == 1:
        axes = axes.reshape(1, -1)
    
    for i in range(batch_size):
        # ì´ë¯¸ì§€
        img_slice = images[i, 0, slice_idx]  # [H, W]
        axes[i, 0].imshow(img_slice, cmap='gray')
        axes[i, 0].set_title(f'Sample {i+1}: Original')
        axes[i, 0].axis('off')
        
        # Ground truth ë§ˆìŠ¤í¬
        mask_slice = masks[i, 0, slice_idx]
        axes[i, 1].imshow(img_slice, cmap='gray', alpha=0.7)
        axes[i, 1].imshow(mask_slice, cmap='Reds', alpha=0.5)
        axes[i, 1].set_title(f'Sample {i+1}: Ground Truth')
        axes[i, 1].axis('off')
        
        # ì˜ˆì¸¡ ë§ˆìŠ¤í¬
        pred_slice = predictions[i, 0, slice_idx]
        axes[i, 2].imshow(img_slice, cmap='gray', alpha=0.7)
        axes[i, 2].imshow(pred_slice, cmap='Blues', alpha=0.5)
        axes[i, 2].set_title(f'Sample {i+1}: Prediction')
        axes[i, 2].axis('off')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"ğŸ“Š Visualization saved: {save_path}")
    
    plt.show()
    return fig


def calculate_dataset_stats(csv_path: str, image_col: str = "path", 
                           mask_col: str = "mask", num_samples: int = 100):
    """ë°ì´í„°ì…‹ í†µê³„ ê³„ì‚°"""
    
    print(f"ğŸ“Š Calculating dataset statistics: {csv_path}")
    
    df = pd.read_csv(csv_path)
    total_samples = len(df)
    
    # ìƒ˜í”Œë§
    sample_size = min(num_samples, total_samples)
    sample_indices = np.random.choice(total_samples, sample_size, replace=False)
    
    image_intensities = []
    mask_ratios = []
    image_shapes = []
    
    valid_samples = 0
    
    for idx in sample_indices:
        try:
            row = df.iloc[idx]
            
            # ì´ë¯¸ì§€ ë¡œë“œ
            image_path = row[image_col]
            mask_path = row[mask_col]
            
            with open(image_path, 'rb') as f:
                image = pickle.load(f)
            with open(mask_path, 'rb') as f:
                mask = pickle.load(f)
            
            # numpy ë³€í™˜
            if not isinstance(image, np.ndarray):
                image = np.array(image)
            if not isinstance(mask, np.ndarray):
                mask = np.array(mask)
            
            # í†µê³„ ê³„ì‚°
            image_intensities.extend(image.flatten())
            mask_ratios.append(np.mean(mask > 0))
            image_shapes.append(image.shape)
            valid_samples += 1
            
        except Exception as e:
            print(f"Error processing sample {idx}: {e}")
            continue
    
    if valid_samples == 0:
        print("âŒ No valid samples found")
        return None
    
    # ê²°ê³¼ ì •ë¦¬
    stats = {
        'total_samples': total_samples,
        'analyzed_samples': valid_samples,
        'image_intensity': {
            'mean': np.mean(image_intensities),
            'std': np.std(image_intensities),
            'min': np.min(image_intensities),
            'max': np.max(image_intensities),
            'percentiles': {
                '1%': np.percentile(image_intensities, 1),
                '5%': np.percentile(image_intensities, 5),
                '95%': np.percentile(image_intensities, 95),
                '99%': np.percentile(image_intensities, 99)
            }
        },
        'mask_statistics': {
            'mean_positive_ratio': np.mean(mask_ratios),
            'std_positive_ratio': np.std(mask_ratios),
            'min_positive_ratio': np.min(mask_ratios),
            'max_positive_ratio': np.max(mask_ratios)
        },
        'image_shapes': {
            'unique_shapes': list(set(image_shapes)),
            'most_common_shape': max(set(image_shapes), key=image_shapes.count)
        }
    }
    
    # í†µê³„ ì¶œë ¥
    print(f"ğŸ“Š Dataset Statistics (based on {valid_samples} samples):")
    print(f"   ğŸ“ˆ Image Intensity:")
    print(f"      Mean: {stats['image_intensity']['mean']:.3f} Â± {stats['image_intensity']['std']:.3f}")
    print(f"      Range: [{stats['image_intensity']['min']:.3f}, {stats['image_intensity']['max']:.3f}]")
    
    print(f"   ğŸ¯ Mask Statistics:")
    print(f"      Positive ratio: {stats['mask_statistics']['mean_positive_ratio']:.3%} Â± {stats['mask_statistics']['std_positive_ratio']:.3%}")
    print(f"      Range: [{stats['mask_statistics']['min_positive_ratio']:.3%}, {stats['mask_statistics']['max_positive_ratio']:.3%}]")
    
    print(f"   ğŸ“ Image Shapes:")
    print(f"      Most common: {stats['image_shapes']['most_common_shape']}")
    print(f"      Unique shapes: {len(stats['image_shapes']['unique_shapes'])}")
    
    return stats


def create_output_directories(config):
    """ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±"""
    dirs_to_create = [
        config.output_dir,
        os.path.join(config.output_dir, 'checkpoints'),
        os.path.join(config.output_dir, 'visualizations'),
        os.path.join(config.output_dir, 'logs')
    ]
    
    for dir_path in dirs_to_create:
        os.makedirs(dir_path, exist_ok=True)
        print(f"ğŸ“ Directory created: {dir_path}")


def log_gpu_memory():
    """GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1e9
        cached = torch.cuda.memory_reserved() / 1e9
        total = torch.cuda.get_device_properties(0).total_memory / 1e9
        
        print(f"ğŸ–¥ï¸ GPU Memory: {allocated:.1f}GB allocated, {cached:.1f}GB cached, {total:.1f}GB total")
        
        return {
            'allocated_gb': allocated,
            'cached_gb': cached,
            'total_gb': total,
            'utilization': allocated / total
        }
    else:
        print("ğŸ–¥ï¸ GPU not available")
        return None


def validate_data_paths(config):
    """ë°ì´í„° ê²½ë¡œ ìœ íš¨ì„± ê²€ì‚¬"""
    errors = []
    warnings = []
    
    # CSV íŒŒì¼ í™•ì¸
    for name, path in [("Training", config.train_csv_path), ("Validation", config.val_csv_path)]:
        if not path:
            errors.append(f"{name} CSV path is empty")
            continue
            
        if not os.path.exists(path):
            errors.append(f"{name} CSV file not found: {path}")
            continue
        
        try:
            df = pd.read_csv(path)
            
            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_cols = ["path", "mask"]
            missing_cols = [col for col in required_cols if col not in df.columns]
            if missing_cols:
                errors.append(f"{name} CSV missing columns: {missing_cols}")
            
            # ìƒ˜í”Œ ìˆ˜ í™•ì¸
            if len(df) == 0:
                errors.append(f"{name} CSV is empty")
            elif len(df) < 10:
                warnings.append(f"{name} CSV has only {len(df)} samples")
            
            print(f"âœ… {name} CSV: {len(df)} samples")
            
        except Exception as e:
            errors.append(f"Error reading {name} CSV: {e}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
    try:
        os.makedirs(config.output_dir, exist_ok=True)
        test_file = os.path.join(config.output_dir, "test_write.tmp")
        with open(test_file, 'w') as f:
            f.write("test")
        os.remove(test_file)
        print(f"âœ… Output directory writable: {config.output_dir}")
    except Exception as e:
        errors.append(f"Output directory not writable: {e}")
    
    # ê²°ê³¼ ë°˜í™˜
    if errors:
        print("âŒ Data validation errors:")
        for error in errors:
            print(f"   - {error}")
        return False
    
    if warnings:
        print("âš ï¸ Data validation warnings:")
        for warning in warnings:
            print(f"   - {warning}")
    
    print("âœ… Data paths validation passed")
    return True


def format_time(seconds: float) -> str:
    """ì‹œê°„ì„ ê°€ë…ì„± ì¢‹ì€ í˜•íƒœë¡œ í¬ë§·"""
    if seconds < 60:
        return f"{seconds:.1f}s"
    elif seconds < 3600:
        return f"{seconds/60:.1f}m"
    else:
        return f"{seconds/3600:.1f}h"


def get_model_size(model) -> Dict[str, Any]:
    """ëª¨ë¸ í¬ê¸° ì •ë³´ ê³„ì‚°"""
    param_count = sum(p.numel() for p in model.parameters())
    trainable_param_count = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì • (bytes)
    param_size = sum(p.numel() * p.element_size() for p in model.parameters())
    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())
    
    total_size_mb = (param_size + buffer_size) / 1e6
    
    return {
        'total_params': param_count,
        'trainable_params': trainable_param_count,
        'frozen_params': param_count - trainable_param_count,
        'size_mb': total_size_mb,
        'trainable_ratio': trainable_param_count / param_count if param_count > 0 else 0
    }


def print_model_info(model, config):
    """ëª¨ë¸ ì •ë³´ ì¶œë ¥"""
    model_info = get_model_size(model)
    
    print("ğŸ§  Model Information:")
    print(f"   ğŸ“Š Total parameters: {model_info['total_params']:,}")
    print(f"   ğŸ”§ Trainable parameters: {model_info['trainable_params']:,}")
    print(f"   â„ï¸ Frozen parameters: {model_info['frozen_params']:,}")
    print(f"   ğŸ“ Model size: {model_info['size_mb']:.1f} MB")
    print(f"   ğŸ”„ Trainable ratio: {model_info['trainable_ratio']:.1%}")
    
    # ëª¨ë“ˆë³„ íŒŒë¼ë¯¸í„° ë¶„ì„
    print("\nğŸ” Module-wise parameter count:")
    for name, module in model.named_children():
        module_params = sum(p.numel() for p in module.parameters())
        trainable_params = sum(p.numel() for p in module.parameters() if p.requires_grad)
        
        if module_params > 0:
            print(f"   {name}: {module_params:,} ({trainable_params:,} trainable)")


if __name__ == "__main__":
    print("Testing SAM utilities...")
    
    # ì‹œë“œ ì„¤ì • í…ŒìŠ¤íŠ¸
    set_seed(42)
    
    # GPU ë©”ëª¨ë¦¬ í™•ì¸
    memory_info = log_gpu_memory()
    
    # ë”ë¯¸ ëª¨ë¸ë¡œ ëª¨ë¸ ì •ë³´ í…ŒìŠ¤íŠ¸
    import torch.nn as nn
    
    class DummyModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.conv = nn.Conv3d(1, 64, 3, padding=1)
            self.fc = nn.Linear(64, 10)
            # ì¼ë¶€ íŒŒë¼ë¯¸í„° freeze
            for param in self.conv.parameters():
                param.requires_grad = False
    
    dummy_model = DummyModel()
    print_model_info(dummy_model, None)
    
    # ì‹œê°„ í¬ë§· í…ŒìŠ¤íŠ¸
    print(f"\nTime formatting test:")
    for seconds in [30, 90, 3660, 7200]:
        print(f"  {seconds}s -> {format_time(seconds)}")
    
    print("\nSAM utilities tests completed!")